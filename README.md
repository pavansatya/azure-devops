# Cloud Fundamentals

This repository contains all the details related to my journey at I Net Software Technologies Inc., my OPT employer, where I was learning the fundamentals of different types of cloud, such as Azure and AWS. 

- First couple of weeks, I got introduced to all the primary resources that Azure offers, such as resource groups, capacities, tenant settings, event hub, and function apps. Followed by setting up a trial account and getting all the permissions under the organisation, managing different users in that organisation by assigning roles and permissions using Role-Based Access Control (RBAC).
- From the third week, I was introduced to the Microsoft Fabric, an all-in-one unified analytics platform by Azure. We started with architectures, ETL, semantic layers, workspaces, Onelake and several other resources that are used in Fabric for the next three weeks.

1) I also started to work with datasets found in Microsoft's official documentation, some given by my supervisor, and a few others from Kaggle, matching the requirements of the problem to gain practical applications on Ingesting data into Lakehouse from external sources like API endpoints, flat files like Comma Separated Files (csv), txt and Excel, and Kusto DB, etc. 

2) We then transformed this ingested data into Databricks notebooks through Dataflow Gen 2 or directly in the Lakehouse, depending on the problem statement, to see how things work for different scenarios. I had to learn a new scripting language, Spark, in the journey to support my transformation processes, which helped me to understand further about large datasets and 
how spark jobs work in real.

3) 
